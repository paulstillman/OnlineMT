

############################### SET UP ###############################

rm(list=ls())

##### Set your directories here #####
# location of code
code.dir = "/Users/jana-mariahohnsbehn/Documents/Online MT try"

# location of raw Qualtrics data
data.dir = "/Users/jana-mariahohnsbehn/Documents/Online MT try"

# where to save key
key.dir = "/Users/jana-mariahohnsbehn/Documents/Online MT try"

# where to save results
results.dir = "/Users/jana-mariahohnsbehn/Documents/Online MT try"


# load helper fns
setwd(code.dir)
source("general_helper.R")

# disable scientific notation because causes big numbers to be treated as equal
options(scipen=999)

# load raw, wide-format Qualtrics data
setwd(data.dir)
d = read.csv("mt_food.csv") # This crucial to be changed with every new study
#ori_mt = read.csv("raw_data.csv")

# remove training trials
d = d[ , -grep( "train", names(d) ) ]
# grep() is a command in R for pattern matching and replacement --> so take all column nemas of d with "train" in thema and remove them

# number of real, non-training stimuli (foods)
n.stim = 18 ## This crucial to be changed with every new study

# stimulus names
stim.names = paste( "food.", 1:n.stim, sep="" ) # This crucial to be changed with every new study


############################### CHECK FOR MISSING DATA ###############################

# the questionnaire is set up so that skipping questions is impossible
# but it's still possible for subjects to stop partway through

# important: this needs to be created as a global variable with this name
# because the subsequent functions in the next code section
# will populate it with subjects that should be excluded
exclusions = data.frame()

# recode missing data
d[ d == "" ] = NA

# check for missing data where there should not be any
cat.names = names(d)[ grep( "_cat", names(d) ) ]
cant.be.na = c( "ResponseId",
                "onReadyTime",
                "buttonClickTime",
                "xPos",
                "yPos",
                "t",
                cat.names )

# mean missingness by variable --> in our test data set we have no NAs
apply( d[ ,names(d) %in% cant.be.na ], 2, function(x) mean(is.na(x) ) )
# apply function: 
  #d[ ,names(d) %in% cant.be.na ] = look at columns in d dataframe but only at those that are also in object cant.be.na
  # the 2 in the command means to perform whatever comes next in the command on the columns (per column/variable). If there would be a 1 instead that would mean to perform the following command (FUN argument) per row
  # function(x) is an anonymous function, but I don't really understand how this works; somehow this is need to access the mean of the NAs

# row numbers for subjects with any missing data
has.na = apply( d[ ,names(d) %in% cant.be.na ], 1, function(x) any( is.na(x) ) )

# exclude them
exclusions = rbind( exclusions, data.frame( ResponseId = d$ResponseId[has.na],
                                            reason = "Missing entire cells of data") )
# this gives me an error message:
# Fehler in data.frame(ResponseId = d$ResponseId[has.na], reason = "Missing entire cells of data") : 
# Argumente implizieren unterschiedliche Anzahl Zeilen: 0, 1
# The question is whether this is coming from the fact that we have no missing data (in has.na all are FALSE)

d = d[ !d$ResponseId %in% exclusions$ResponseId, ]


############################### MAKE STIMULUS-URL KEY FOR RANDOMIZED LOOP & MERGE ###############################

make_url_key( dat = d,
             n.stim = n.stim,
             stim.names = stim.names,
            lm.varname = "cat", 
            key.dir = key.dir )

# read in the key 
setwd(key.dir)
key = read.csv("autogenerated_stimulus_vs_url_key.csv")

# the URLS key is exported with the data per looped stimulus. It is stores in the row with the qualtrics label importID, this is why after mkaing it we can delete this row
# remove extra header rows that we used for making key above
d = d[3:nrow(d),]

# sample size
( initial.n = nrow(d) )

# The sample size of our test sample is 20

####################### TOTAL MINUTES SUBJECTS SPENT AND HOURLY PAY RATE #######################

# mean experiment duration in minutes
summary( as.numeric( d[["Duration..in.seconds."]] ) / 60 )
 # In our test sample, mean was 3.06 minutes

# Don't need the following hourly rate thing for the test case
# median hourly pay rate
#min = median( as.numeric( d[["Duration (in seconds)"]] ) / 60 )
# amount we paid on MTurk
#cents = 0.25
#( 60 / min ) * cents # hourly pay rate



############################### REMOVE BAD DATA ###############################

# this requires actually trying to make the subject lists for 1 coordinate variable
#  in order to check for problems

# dry run to see which subjects need to be excluded --> Her I absolute don't know what is happening
invisible( get_subject_lists( data = d, 
                        var.name = "xPos",
                        reorder = TRUE,
                        key = key ) )

# see how many need to be excluded for each reason
table(exclusions$reason)

# remove them
d = d[ !d$ResponseId %in% exclusions$ResponseId, ]



############################### WHAT ALERTS DID SUBJECTS GET? ###############################

alerts = describe_alerts( dat = d,
                          n.stim = n.stim,
                          key = key )
# describe_alerts is a function she wrote herself
# When looking at the alerts object, it says that in our test sample 94% of participants received no alerts and 5% received the alert 1 
# 1 = 'Started too early';

############################### PARSE CURSOR DATA ###############################

# lists for each of the three spacetime variables
# these will be ordered correctly, not randomized

# so the [[i]][[j]] entry of each of these is the ith subject's vector
#  of values for stimulus j (where j is the order in the key, not the randomized order
#  it was shown)

# rescale so that every face trajectory has length 1 in x and y direction

# x-coordinates
xl = get_subject_lists( data = d, 
                        var.name = "xPos",
                        reorder = TRUE,
                        key = key,
                        rescale = TRUE )

# y-coordinates
yl = get_subject_lists( data = d, 
                        var.name = "yPos",
                        reorder = TRUE,
                        key = key,
                        rescale = TRUE )

# time coordinates
tl = get_subject_lists( data = d, 
                        var.name = "time",
                        reorder = TRUE,
                        key = key )

# add outcome variables to wide-format data
d = add_outcomes( d, xl, yl, tl )


# save R objects
setwd(data.dir)
save( xl, file = "xl_subject_lists.RData" )
save( yl, file = "yl_subject_lists.RData" )
save( tl, file = "tl_subject_lists.RData" )



####################### NUISANCE COVARIATES THAT WILL BE ADJUSTED IN ANALYSIS #######################

# identify subjects who ever had window too small
rows = grep( "4", d$alerts ) #--> there should we nothing in the rows object since we only had alerts 0 and 1

# make indicator variable for having window too small
d$wts = FALSE
d$wts[ rows ] = TRUE
table(d$wts)

# it says FALSE:20 --> none of our 20 test responses had the window too small --> I think this becomes a problem for the later analysis:
# if there no variance at all this variable cannot be processed in the later models because it has only one level. This why for the test sample I removed it

# identify subjects with pixel scaling issues
weird.scalers = exclusions$ResponseId[ exclusions$reason == "Nonstandard pixel dimensions." ]
d$weird.scaling = FALSE
d$weird.scaling[ d$ResponseId %in% weird.scalers ] = TRUE 
table(d$weird.scaling)
# 5 responses in our test sample had scaling issues --> should they be excluded? What does say in their paper about that?


####################### MORE SUBJECT EXCLUSIONS #######################

# if you want to exclude any additional subjects, do it here

# Are theere any more exclusion criteria that were pre-registered for the original mt study in the lab?

# save final wide dataset 
setwd(data.dir)
write.csv( d, "wide_data_prepped.csv", row.names = FALSE )

# final sample size
nrow(d)
# Our final sample is 20

####################### RESHAPE INTO LONG FORM #######################

# for analysis, we want a long-form dataset with 1 row per trial
# each row represents one unique stimulus and subject combo

l = wide_to_long( dat = d, 
                  stim.names = stim.names )

setwd(data.dir)
write.csv(l, "long_data_prepped.csv")

# write all other R objects as well
setwd(results.dir)
save.image( file = "all_data_prep_objects.RData")


